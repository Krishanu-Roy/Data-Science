Module 1: 
What is Data
# Topic Objective: Identify applications of data in daily life.

Data is created through many modern daily activities. Organizations gather this data and apply data analytics to inform practical business applications. 
There are three main types of data: observed data, volunteered data, and inferred data. The correct data visualization can intuitively present complex data patterns and trends.  

The main factors to consider when choosing a visualization are:  

> The number of variables to be shown.  

> The number of data points, or units of information, in each variable.  

> Whether the data illustrates changes over time.  

> The need to make a comparison or correlation between different groups of data points.  

Data is All Around Us !

Topic objective: Compare and contrast the different types of data.

Data analysis begins with understanding the types of data. Data is either defined as quantitative or qualitative. Quantitative data is divided into discrete and 
continuous data. The data type tells a system how to interpret the data’s value, so the system can perform operations to transform and use the data in computations.  

Data types include:  
String , Integer , Floating point , Date and time , Boolean 

The data should be categorized as structured or unstructured before it is processed, stored, and analyzed. After data is defined and selected, 
it becomes relevant in determining the questions to be answered.  

Business Understanding 

Topic Objective: Evaluate the value gained through analytics.

Data science enables businesses to better understand the impact of their products and services, adjust their methods and goals, and provide their customers with 
better products faster. Trend analysis is one way to gain insights into key performance indicators (KPI) over time.  

Humanitarian Organizations use data to serve their communities and the world. Predictive analytics can focus humanitarian efforts on preventive 
rather than reactive actions. Environmental agencies track climate change data through observations which enable predictions of societal impact. 

REFLECTION 

The world population continues to climb. The need to feed that growing population makes the agricultural sector more dependent on data than ever before. 
Agricultural farmers utilize data to determine the right weather, soil yield, supply and demand for the type of crops. These analytics impact what to plant, when to plant,
when to harvest, and when to sell. For livestock farmers, data analytics drives breeding rates, land management, purchasing of hay and grain, and determining the sales
market. 

The farmer's role may have stayed relatively the same as the decades before us, but the approach has drastically changed due to data analytics. 
The increased need for a sustainable and consistent food supply raises new questions. Questions for reflection include:  

What percentage of land should be dedicated to farming versus communities?  

What other data should be gathered to ensure a good food source for generations while not negatively impacting the environment?  

Data analytics will continue to play a significant role in improving farming methods for future generations. 

Module 2: 

Defining Big Data
The data analysts at Data Crunchers introduced you to the concepts of analytics and the importance of visualizations. Now it is time to work alongside the company’s data engineers to learn about big data and the role that engineers play in building, maintaining, and ensuring that the organization’s data infrastructure is available and reliable. 

Big data is a term used to describe the massive volumes of digital data generated, collected, and processed. The term big data describes data that is either moving too quickly, is simply too large, or is too complex to be stored, processed, or analyzed with traditional data storage and analytics applications. Some examples of big data include data generated by postings to social media accounts, such as Facebook and Twitter, and the ratings given to products on e-commerce sites like Amazon marketplace. 

Size is only one of the characteristics that define big data. Other criteria include the speed of generated data and the variety of data collected and stored. 

Big Data Characteristics
Selectable image component. Select each item to show more information.
Big data's characteristics change how data is collected, transmitted, stored, and accessed. Click on each quadrant in the figure below to read about the four Vs of big data and the challenges they create for the data infrastructure engineers. 

> Volume
> Variety
> Velocity
> Veracity

 Data Pipelines
Using all of this data to achieve these potential benefits requires managing the data. Data engineers are the professionals who engage in this management. This process includes developing infrastructure and systems to ingest the data, clean and transform it, and finally store it in ways that make it easy for the rest of the people in their organization to access and query the data to answer business questions. 

What is a data pipeline?  

The best approach is to think of a data pipeline to understand better what data engineers do with data. You can think of it almost like water flowing through pipes. To understand what data engineers do with these data, consider the figure below, which is a simplified representation of data flowing through the three phases of a data pipeline: ingestion, transformation, and storage.  

Note: You will also see the acronym ETL, which stands for Extract, Transform, and Load. Extract is equivalent to Ingestion and Storage is equivalent to Load.

Ingestion
Data engineers will want to ingest two primary sources of data: batches of data from servers or databases (batch ingestion) and real-time events happening in the world and streaming from the world of devices (streaming ingestion). An example of batch ingestion is a game company that wants to examine the relationship between subscription renewals and customer support tickets. It could ingest all the related data on a daily or weekly basis. It doesn’t need to access and analyze data immediately after a support ticket is closed or a subscription is renewed. An example of streaming ingestion is when you request a ride from a ride share service. The company combines streams of data (e.g. historical data, real-time traffic data, and location tracking) to make sure you get a ride from the driver who is closest to you at the time. 

The Machine Learning Process
Developing a machine learning solution is seldom a linear process. Several trial-and-error steps are necessary to fine-tune the solution. The details of each step performed by the Data Crunchers data scientists as they work on the new weed identification and eradication model are as follows:

Step 1. Data preparation - Perform data cleaning procedures such as transformation into a structured format and removing missing data and noisy/corrupted observations.   

Step 2a. Learning data - Create a learning data set used to train the model. 

Step 2b. Testing data - Create a test dataset used to evaluate the model performance. Only perform this step in the case of supervised learning.   

Step 3. Learning Process Loop - Selection. An algorithm is chosen based on the problem. Depending on the selected algorithm, additional pre-processing steps might be necessary.

Step 4. Learning Process Loop - Evaluation. This selected algorithm's performance is evaluated on the learning data. If the algorithm and the model reach an acceptable performance on learning data, the solution validates the test data. Otherwise, repeat the learning process with a proposed new model and algorithm.

Step 5. Model evaluation - Test the solution on the test data. The performances on learning data are not necessarily transferrable to test data. The more complex and fine-tuned the model is, the higher the chances are that the model will become prone to overfitting, which means it cannot perform accurately against unseen data. Overfitting can result in going back to the model learning process.   

Step 6. Model implementation - After the model achieves satisfactory performance on test data, implement the model. Implementing the model means performing the necessary tasks to scale the machine learning solution to big data.  
